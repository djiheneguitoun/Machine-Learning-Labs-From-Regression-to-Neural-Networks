{"cells":[{"cell_type":"markdown","metadata":{"id":"FqoNBvfeUk15"},"source":["# 2CS-SIL2/SIQ2 Lab04. Naïve Bayes\n","\n","<p style='text-align: right;font-style: italic;'>Designed by: Mr. Abdelkrime Aries</p>\n","\n","In this lab, we will learn about Naive Bayes by testing 2 implementations:\n","- Multinomial Naïve Bayes\n","- Gaussian Naïve Bayes"]},{"cell_type":"markdown","metadata":{"id":"OpvZWj_JUk1-"},"source":["**Team:**\n","- **Member 01**: ...\n","- **Member 02**: ...\n","- **Group**: SIL2/SIQ2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"bToWZxPxUk2A","executionInfo":{"status":"ok","timestamp":1745790450586,"user_tz":-60,"elapsed":260,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"ea7aee3d-6bcc-486f-dfc0-d47e19f3e295"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["import sys, timeit\n","from typing          import Tuple, List, Type\n","from collections.abc import Callable\n","\n","sys.version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vSvogegxUk2B","executionInfo":{"status":"ok","timestamp":1745790454254,"user_tz":-60,"elapsed":997,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"787b1a9e-1c13-46e4-eed5-01777042997d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('2.0.2', '2.2.2', '3.10.0')"]},"metadata":{},"execution_count":3}],"source":["import numpy             as np\n","import pandas            as pd\n","import matplotlib.pyplot as plt\n","import matplotlib\n","%matplotlib inline\n","\n","np.__version__, pd.__version__, matplotlib.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"35-F-8zYUk2D","executionInfo":{"status":"ok","timestamp":1745790458136,"user_tz":-60,"elapsed":2211,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"26e4425f-f45e-47ad-d29b-c44292854f18"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.6.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["import sklearn\n","\n","from sklearn.naive_bayes   import CategoricalNB\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.metrics       import classification_report\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection         import train_test_split\n","from sklearn.naive_bayes             import MultinomialNB, GaussianNB\n","from sklearn.linear_model            import LogisticRegression\n","from sklearn.tree                    import DecisionTreeClassifier\n","from sklearn.metrics                 import precision_score, recall_score\n","import timeit\n","\n","\n","sklearn.__version__"]},{"cell_type":"markdown","metadata":{"id":"DxC_rXXNUk2E"},"source":["## I. Algorithms implementation\n","\n","In this section, we will try to implement multinomial Naive Bayes.\n","\n","\n","**>> Try to use \"numpy\" which will save a lot of time and effort**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4CQuOcWOUk2F","executionInfo":{"status":"ok","timestamp":1745790460292,"user_tz":-60,"elapsed":284,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"da7bf17d-a963-401b-b63a-03990853f0aa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(14, 14)"]},"metadata":{},"execution_count":5}],"source":["# Dataset play\n","\n","# outlook & temperature & humidity & windy\n","Xplay = np.array([\n","    ['sunny'   , 'hot' , 'high'  , 'no'],\n","    ['sunny'   , 'hot' , 'high'  , 'yes'],\n","    ['overcast', 'hot' , 'high'  , 'no'],\n","    ['rainy'   , 'mild', 'high'  , 'no'],\n","    ['rainy'   , 'cool', 'normal', 'no'],\n","    ['rainy'   , 'cool', 'normal', 'yes'],\n","    ['overcast', 'cool', 'normal', 'yes'],\n","    ['sunny'   , 'mild', 'high'  , 'no'],\n","    ['sunny'   , 'cool', 'normal', 'no'],\n","    ['rainy'   , 'mild', 'normal', 'no'],\n","    ['sunny'   , 'mild', 'normal', 'yes'],\n","    ['overcast', 'mild', 'high'  , 'yes'],\n","    ['overcast', 'hot' , 'normal', 'no'],\n","    ['rainy'   , 'mild', 'high'  , 'yes']\n","])\n","\n","Yplay = np.array([\n","    'no',\n","    'no',\n","    'yes',\n","    'yes',\n","    'yes',\n","    'no',\n","    'yes',\n","    'no',\n","    'yes',\n","    'yes',\n","    'yes',\n","    'yes',\n","    'yes',\n","    'no'\n","])\n","\n","len(Xplay), len(Yplay)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0CXvw1zUk2F","executionInfo":{"status":"ok","timestamp":1745790463761,"user_tz":-60,"elapsed":463,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"563fb911-b6a6-466c-ce04-ad9bc570e1ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 8)"]},"metadata":{},"execution_count":6}],"source":["# height & weight & footsize & person\n","Xperson = np.array([\n","    [182., 81.6, 30.],\n","    [180., 86.2, 28.],\n","    [170., 77.1, 30.],\n","    [180., 74.8, 25.],\n","    [152., 45.4, 15.],\n","    [168., 68.0, 20.],\n","    [165., 59.0, 18.],\n","    [175., 68.0, 23.]\n","])\n","\n","Yperson = np.array([\n","    'male', 'male', 'male', 'male',\n","    'female', 'female', 'female', 'female'\n","])\n","\n","len(Xperson), len(Yperson)"]},{"cell_type":"markdown","metadata":{"id":"oLfpcbpxUk2G"},"source":["### I.1. Prior statistics\n","\n","Given an output list $Y[M]$, the probability of each class $c$ is estimated as:\n","$$p(c) = \\frac{\\#(Y = c)}{|Y|}$$\n","\n","In here, we want to store the frequencies of different classes.\n","Our function must return two lists:\n","- One containing the names of unique classes.\n","- Another containing their frequencies."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-pyOO7DUk2H","executionInfo":{"status":"ok","timestamp":1745790467497,"user_tz":-60,"elapsed":272,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"0ef0f4fe-c081-4071-be9a-2ce9d324ec56"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((array(['no', 'yes'], dtype='<U3'), array([5, 9])),\n"," (array(['female', 'male'], dtype='<U6'), array([4, 4])))"]},"metadata":{},"execution_count":7}],"source":["# TODO: Prior statistics\n","def fit_prior(Y: 'np.ndarray[M](str)') -> Tuple['np.ndarray[K](str)', 'np.ndarray[K](int)']:\n","    return np.unique(Y, return_counts=True)\n","\n","#=====================================================================\n","# UNIT TEST\n","#=====================================================================\n","# Result:\n","# ((array(['no', 'yes'], dtype='<U3'), array([5, 9])),\n","#  (array(['female', 'male'], dtype='<U6'), array([4, 4])))\n","#---------------------------------------------------------------------\n","\n","fit_prior(Yplay), fit_prior(Yperson)"]},{"cell_type":"markdown","metadata":{"id":"4N-qvsJOUk2I"},"source":["### I.2. Multinomial Law\n","\n","In this section, we will implement multinomial naive Bayes from scratch using Numpy.\n","\n","#### I.2.1. Multinomial Likelihood statistics\n","\n","Given:\n","- $A$: a categorical feature\n","- $Y$: the ouput\n","- $C$: the classes\n","\n","The function takes as argument $A, Y, C$ previously described.\n","It must return:\n","- $V$: unique values of this feature (feature's categories)\n","- $S[|C|, |V|]$: a matrix containing count $\\#(Y = c \\wedge A = v),\\, \\forall c \\in C, \\forall v \\in A$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W46lSAGZUk2J","executionInfo":{"status":"ok","timestamp":1745790473039,"user_tz":-60,"elapsed":288,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"0491b73b-84ef-4e44-dd6c-73738c383023"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n","  array([[0, 2, 3],\n","         [4, 3, 2]])),\n"," (array(['cool', 'hot', 'mild'], dtype='<U8'),\n","  array([[1, 2, 2],\n","         [3, 2, 4]])))"]},"metadata":{},"execution_count":8}],"source":["# TODO: Multinomial Likelihood statistics\n","def fit_multinomial_likelihood(A: 'np.ndarray[M](str)',\n","                               Y: 'np.ndarray[M](str)',\n","                               C: 'np.ndarray[C](str)'\n","                               ) -> Tuple['np.ndarray[V](str)', 'np.ndarray[C, V](int)']:\n","    V = np.unique(A)  # classe dune colonne (attribut)\n","    S = np.zeros((len(C), len(V)), dtype=int)  # Matrice |C| x |V| remplie de zéros\n","    #C= les classe de output\n","    # Y= output\n","    for i, c in enumerate(C):\n","        for j, v in enumerate(V):\n","            # Compte le nombre d'éléments où Y==c et A==v\n","            S[i, j] = np.sum((Y == c) & (A == v))\n","\n","    return V, S\n","\n","\n","#=====================================================================\n","# UNIT TEST\n","#=====================================================================\n","# Result:\n","# ((array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n","#   array([[0, 2, 3],\n","#          [4, 3, 2]])),\n","#  (array(['cool', 'hot', 'mild'], dtype='<U8'),\n","#   array([[1, 2, 2],\n","#          [3, 2, 4]])))\n","#---------------------------------------------------------------------\n","C_t = np.array(['no', 'yes'])\n","fit_multinomial_likelihood(Xplay[:, 0], Yplay, C_t), fit_multinomial_likelihood(Xplay[:, 1], Yplay, C_t)"]},{"cell_type":"markdown","metadata":{"id":"ZPsmq3hEUk2J"},"source":["#### I.2.2. Multinomial Likelihood training\n","\n","**Nothing to code here, although you have to know how it functions for next use**\n","\n","This function aims to generate parameters $\\theta$.\n","In our case, paramters are diffrent from those of *logistic regrssion*.\n","They are a dictionary (map) with two entries:\n","- \"prior\": a dictionary having \"vocab\" a list of values and \"freq\" a list of their respective frequencies.\n","- \"likelihood\": a list of dictionaries representing statistics of each feature (the same order of $X$ features)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNwOW6BWUk2J","executionInfo":{"status":"ok","timestamp":1745790478760,"user_tz":-60,"elapsed":276,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"09678332-a6b5-46a8-b0e2-efd08f4877df"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'prior': {'vocab': array(['no', 'yes'], dtype='<U3'), 'freq': array([5, 9])},\n"," 'likelihood': [{'vocab': array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n","   'freq': array([[0, 2, 3],\n","          [4, 3, 2]])},\n","  {'vocab': array(['cool', 'hot', 'mild'], dtype='<U8'),\n","   'freq': array([[1, 2, 2],\n","          [3, 2, 4]])},\n","  {'vocab': array(['high', 'normal'], dtype='<U8'),\n","   'freq': array([[4, 1],\n","          [3, 6]])},\n","  {'vocab': array(['no', 'yes'], dtype='<U8'),\n","   'freq': array([[2, 3],\n","          [6, 3]])}]}"]},"metadata":{},"execution_count":9}],"source":["def fit_multinomial_NB(X: 'np.ndarray[M, N](str)',\n","                       Y: 'np.ndarray[M](str)'\n","                       ) -> object:\n","\n","    Theta   = {'prior': {}, 'likelihood': []}\n","\n","    Theta['prior']['vocab'], Theta['prior']['freq'] = fit_prior(Y)\n","\n","    for j in range(X.shape[1]):\n","        likelihood = {}\n","        likelihood['vocab'], likelihood['freq'] = fit_multinomial_likelihood(X[:, j], Y, Theta['prior']['vocab'])\n","        Theta['likelihood'].append(likelihood)\n","\n","    return Theta\n","\n","\n","#=====================================================================\n","# UNIT TEST\n","#=====================================================================\n","# Result:\n","# {'prior': {'vocab': array(['no', 'yes'], dtype='<U3'), 'freq': array([5, 9])},\n","#  'likelihood': [{'vocab': array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n","#    'freq': array([[0, 2, 3],\n","#           [4, 3, 2]])},\n","#   {'vocab': array(['cool', 'hot', 'mild'], dtype='<U8'),\n","#    'freq': array([[1, 2, 2],\n","#           [3, 2, 4]])},\n","#   {'vocab': array(['high', 'normal'], dtype='<U8'),\n","#    'freq': array([[4, 1],\n","#           [3, 6]])},\n","#   {'vocab': array(['no', 'yes'], dtype='<U8'),\n","#    'freq': array([[2, 3],\n","#           [6, 3]])}]}\n","#---------------------------------------------------------------------\n","Theta_play = fit_multinomial_NB(Xplay, Yplay)\n","\n","Theta_play"]},{"cell_type":"markdown","metadata":{"id":"pdp375VPUk2K"},"source":["#### I.2.3. Multinomial Likelihood prediction\n","\n","Given:\n","- $A$: a categorical feature\n","- $V$: unique values of this feature (feature's categories)\n","- $Y$: the ouput\n","- $C$: the classes\n","- $\\alpha$: smoothing factor\n","\n","Log likelihood is calculated as:\n","$$ \\log p(A=v|Y=c) = \\log(\\#(Y = k \\wedge A = v) + \\alpha) - \\log(\\#(y = k) + \\alpha * |V|)$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jtF8D0qUk2K","executionInfo":{"status":"ok","timestamp":1745790483736,"user_tz":-60,"elapsed":271,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"a940f27c-1124-4b72-f027-fe51da754da2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(np.int64(1), -1)"]},"metadata":{},"execution_count":10}],"source":["# You can use this function in the next implimentation\n","# It takes a list of unique values V and a given value v\n","# It returns the position of v in V\n","# If v does not exist in V, it rturns -1\n","def find_idx(V: np.ndarray, v: str) -> int:\n","    k = np.argwhere(V == v).flatten()\n","    if len(k):\n","        return k[0]\n","    return -1\n","\n","V_t = np.array(['One', 'Two', 'Three'])\n","find_idx(V_t, 'Two'), find_idx(V_t, 'Four')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5lW7E6T9Uk2K","executionInfo":{"status":"ok","timestamp":1745790486567,"user_tz":-60,"elapsed":306,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"739bea7f-93a7-4fae-9929-e6ed5bc37d7a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([-0.91629073, -1.09861229]), array([-2.07944154, -2.48490665]))"]},"metadata":{},"execution_count":11}],"source":["def predict_multinomial_NB1(v: str,\n","                            j: int,\n","                            Theta: object,\n","                            alpha: float = 0.\n","                            ) -> 'np.ndarray[C](float)':\n","\n","    prior_freq = np.array(Theta['prior']['freq'])  # Conversion liste -> numpy array\n","    index_V = find_idx(Theta['likelihood'][j]['vocab'], v)\n","\n","    if index_V == -1:\n","        result = np.log(alpha) - np.log(prior_freq + alpha * len(Theta['likelihood'][j]['vocab']))\n","    else:\n","        result = np.log(Theta['likelihood'][j]['freq'][:, index_V] + alpha) - np.log(prior_freq + alpha * len(Theta['likelihood'][j]['vocab']))\n","\n","    return result\n","\n","\n","\n","\n","#=====================================================================\n","# UNIT TEST\n","#=====================================================================\n","# Result:\n","# (array([-0.91629073, -1.09861229]), array([-2.07944154, -2.48490665]))\n","#---------------------------------------------------------------------\n","\n","X_t = np.array([\n","    ['rainy', 'cool', 'normal', 'yes'],\n","    ['snowy', 'cool', 'normal', 'yes'],\n","    ['sunny', 'hot' , 'normal', 'no']\n","])\n","\n","predict_multinomial_NB1('rainy', 0, Theta_play, alpha=0.), \\\n","    predict_multinomial_NB1('snowy', 0, Theta_play, alpha=1.)"]},{"cell_type":"markdown","metadata":{"id":"pkodyXjUUk2K"},"source":["### I.3. Normal (Gaussian) Law\n","\n","In this section, we will implement gaussian naive Bayes from scratch using Numpy.\n","\n","#### I.3.1. Gaussian Likelihood statistics\n","\n","Given:\n","- $A$: a categorical feature\n","- $Y$: the ouput\n","- $C$: the classes\n","\n","The function takes as argument $A, Y, C$ previously described.\n","It must return $S[|C|, 2, N]$; a tensor having these dimensions:\n","- first dimension: each element represents one class's statistics\n","- second dimension: 1st element represents means; 2ns element represents variances\n","- third dimension: each element represents mean/variance of the respective feature"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZC6adbLUk2K","executionInfo":{"status":"ok","timestamp":1745790495540,"user_tz":-60,"elapsed":286,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"c8b155ab-de9f-44b5-95fd-1f1553616816"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[165.        ,  60.1       ,  19.        ],\n","        [ 92.66666667, 114.04      ,  11.33333333]],\n","\n","       [[178.        ,  79.925     ,  28.25      ],\n","        [ 29.33333333,  25.47583333,   5.58333333]]])"]},"metadata":{},"execution_count":12}],"source":["# TODO: Gaussian Likelihood statistics\n","def fit_gaussian_likelihood(X: 'np.ndarray[M, N](float)',\n","                            Y: 'np.ndarray[M](str)',\n","                            C: 'np.ndarray[C](str)'\n","                            ) -> Tuple['np.ndarray[C, 2, N](float)']:\n","\n","    D = np.zeros((len(C),2,X.shape[1]))\n","    for index, c in enumerate(C):\n","        XC = X[Y==c]\n","        for x in range(X.shape[1]):\n","            D[index][0][x] = np.mean(XC[:, x])\n","            D[index][1][x] = np.var(XC[:, x], ddof=1)\n","    return D\n","\n","\n","#look here rosa incorect version\n","   # C_len = len(C)         # Nombre de classes\n","   # N = X.shape[1]         # Nombre d'attributs\n","   # result = np.zeros((C_len, 2, N))  # Initialiser tableau C x 2 x N\n","\n","   # for i, c in enumerate(C):\n","   #     X_c = X[Y == c]   # Échantillons de la classe c\n","\n","   #     result[i, 0, :] = np.mean(X_c, axis=0)     # Moyenne par colonne\n","   #     result[i, 1, :] = np.var(X_c, axis=0)      # Variance par colonne\n","\n","  #  return result\n","#=====================================================================\n","# UNIT TEST\n","#=====================================================================\n","# Result:\n","# array([[[165.        ,  60.1       ,  19.        ],\n","#         [ 92.66666667, 114.04      ,  11.33333333]],\n","\n","#        [[178.        ,  79.925     ,  28.25      ],\n","#         [ 29.33333333,  25.47583333,   5.58333333]]])\n","#---------------------------------------------------------------------\n","C_t = np.array(['female', 'male'])\n","fit_gaussian_likelihood(Xperson, Yperson, C_t)"]},{"cell_type":"markdown","metadata":{"id":"uftw6vqUUk2L"},"source":["#### I.3.2. Gaussian Likelihood training\n","\n","**Nothing to code here, although you have to know how it functions for next use**\n","\n","This function aims to generate parameters $\\theta$.\n","In our case, paramters are diffrent from those of *logistic regrssion*.\n","They are a dictionary (map) with two entries:\n","- \"prior\": a dictionary having \"vocab\" a list of values and \"freq\" a list of their respective frequencies.\n","- \"likelihood\": a tensor of shape $[|C|, 2, N]$ containing likelihood statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwNtUhrYUk2L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745790503183,"user_tz":-60,"elapsed":265,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"640f9302-8a96-4022-8392-0eaaa8296c5d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'prior': {'vocab': array(['female', 'male'], dtype='<U6'),\n","  'freq': array([4, 4])},\n"," 'likelihood': array([[[165.        ,  60.1       ,  19.        ],\n","         [ 92.66666667, 114.04      ,  11.33333333]],\n"," \n","        [[178.        ,  79.925     ,  28.25      ],\n","         [ 29.33333333,  25.47583333,   5.58333333]]])}"]},"metadata":{},"execution_count":13}],"source":["def fit_gaussian_NB(X: 'np.ndarray[M, N](str)',\n","                    Y: 'np.ndarray[M](str)'\n","                    ) -> object:\n","\n","    Theta   = {'prior': {}, 'likelihood': []}\n","\n","    Theta['prior']['vocab'], Theta['prior']['freq'] = fit_prior(Y)\n","    Theta['likelihood'] = fit_gaussian_likelihood(X, Y, Theta['prior']['vocab'])\n","\n","    return Theta\n","\n","\n","#=====================================================================\n","# UNIT TEST\n","#=====================================================================\n","# Result:\n","# {'prior': {'vocab': array(['female', 'male'], dtype='<U6'),\n","#   'freq': array([4, 4])},\n","#  'likelihood': array([[[165.        ,  60.1       ,  19.        ],\n","#          [ 92.66666667, 114.04      ,  11.33333333]],\n","\n","#         [[178.        ,  79.925     ,  28.25      ],\n","#          [ 29.33333333,  25.47583333,   5.58333333]]])}\n","#---------------------------------------------------------------------\n","Theta_person = fit_gaussian_NB(Xperson, Yperson)\n","\n","Theta_person"]},{"cell_type":"markdown","metadata":{"id":"moMtzVfoUk2M"},"source":["#### I.2.4. Gaussian Likelihood prediction\n","\n","Given:\n","- $A$: a numerical feature\n","- $\\mu_{Ac}$: mean of values of feature $A$ having $c$ as class\n","- $\\sigma_{Ac}$: variance of values of feature $A$ having $c$ as class\n","- $Y$: the output\n","- $C$: the classes\n","\n","Log likelihood is calculated as:\n","$$ \\log p(A=v|Y=c) = \\frac{-(v-\\mu_{Ac})^2}{2 \\sigma_{Ac}^2} - \\log(\\sqrt{2\\pi \\sigma_{Ac}^2})$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qWfPMgQ_Uk2M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745790896117,"user_tz":-60,"elapsed":304,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"4e55ce54-2f3c-42d1-9ccb-c79aa23555bd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([-4.93164438, -3.03443716]), array([0.00721463, 0.04810173]))"]},"metadata":{},"execution_count":15}],"source":["def predict_gaussian_NB1(v: float,\n","                         j: int,\n","                         Theta: object,\n","                         alpha: float = 0.  # unused for Gaussian, kept for compatibility\n","                         ) -> 'np.ndarray[C](float)':\n","\n","    means = Theta['likelihood'][:, 0, j]\n","    variances = Theta['likelihood'][:, 1, j]\n","\n","\n","    log_likelihoods = []\n","    for c in range(len(means)):\n","        mu = means[c]\n","        sigma2 = variances[c]\n","\n","\n","        if sigma2 == 0:\n","            if v == mu:\n","\n","                log_prob = 0.0  # log(1) = 0\n","            else:\n","\n","                log_prob = -np.inf\n","        else:\n","            # Gaussian log likelihood formula:\n","            log_prob = -((v - mu) ** 2) / (2 * sigma2) - 0.5 * np.log(2 * np.pi * sigma2)\n","\n","        log_likelihoods.append(log_prob)\n","\n","    return np.array(log_likelihoods)\n","\n","\n","#=====================================================================\n","# UNIT TEST\n","#=====================================================================\n","# Result:\n","# (array([-4.93164438, -3.03443716]), array([0.00721463, 0.04810173]))\n","#---------------------------------------------------------------------\n","\n","pp = predict_gaussian_NB1(183, 0, Theta_person)\n","\n","pp, np.exp(pp)"]},{"cell_type":"markdown","metadata":{"id":"FnR0npZbUk2M"},"source":["### I.4. Final prediction\n","\n","Our goal is to calculate approximate log probabilities of all classes given a sample:\n","$$\\log P(y=c_k | \\overrightarrow{x} = \\overrightarrow{f})  \\approx \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j = x_j|y=c_k)$$\n","\n","This function takes:\n","- $X^{(i)}$ one sample with $N$ features\n","- $\\theta$ parameters (either those of multinomial or gaussian)\n","- $pred_{fct}$ a function to predict one feauture (either multinomial or gaussian)\n","- add_prior: if True, add prior probability\n","- $\\alpha$ smoothing factor (passing it to gaussian function will do nothing)\n","\n","It must return a vector of probabilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HES8RkMEUk2M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745791033476,"user_tz":-60,"elapsed":364,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"307b1312-c924-477b-92b8-86b153945d3d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([-3.59617006, -4.95406494]),\n"," array([-2.56655064, -4.51223219]),\n"," array([-2.85774653, -4.23617476]),\n"," array([-10.401093  , -22.03977023]))"]},"metadata":{},"execution_count":16}],"source":["# TODO: Final prediction\n","def predict_NB1(Xi: 'np.ndarray[N]',\n","                Theta: object,\n","                pred_fct: Callable,\n","                add_prior: bool = True,\n","                alpha: float = 1.0\n","                ) -> 'np.ndarray[C](float)':\n","\n","    if add_prior:\n","\n","        log_probs = np.log(Theta['prior']['freq'] / np.sum(Theta['prior']['freq']))\n","    else:\n","\n","        log_probs = np.zeros(len(Theta['prior']['vocab']))\n","\n","\n","    for j in range(len(Xi)):\n","\n","        feature_log_probs = pred_fct(Xi[j], j, Theta, alpha)\n","        valid_probs = np.isfinite(feature_log_probs)\n","        log_probs[valid_probs] += feature_log_probs[valid_probs]\n","\n","    return log_probs\n","\n","#=====================================================================\n","# UNIT TEST\n","#=====================================================================\n","# Result:\n","# (array([-2.20940778, -3.86937505]),\n","#  array([-2.56655064, -4.51223219]),\n","#  array([-2.85774653, -4.23617476]),\n","#  array([-10.401093  , -22.03977023]))\n","#---------------------------------------------------------------------\n","\n","X_t1 = np.array(['sunny', 'hot' , 'high', 'no'])\n","X_t2 = np.array([183., 59., 20.])\n","\n","predict_NB1(X_t1, Theta_play, predict_multinomial_NB1, add_prior=True, alpha=0.0), \\\n","predict_NB1(X_t1, Theta_play, predict_multinomial_NB1, add_prior=False, alpha=0.0), \\\n","predict_NB1(X_t1, Theta_play, predict_multinomial_NB1, add_prior=False, alpha=1.0), \\\n","predict_NB1(X_t2, Theta_person, predict_gaussian_NB1, add_prior=False),"]},{"cell_type":"markdown","metadata":{"id":"XdlS2kV9Uk2M"},"source":["### I.5. Final product\n","\n","**>> Nothing to code here**\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"2mYJntHxUk2M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745792891022,"user_tz":-60,"elapsed":284,"user":{"displayName":"BELGACEM ROZA","userId":"07138494401820393416"}},"outputId":"20ddfaae-59c3-4627-a133-df7512ee0a68"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array(['yes', 'yes', 'no'], dtype='<U3'),\n"," array([[-5.20912179, -4.10264337],\n","        [-6.30773408, -5.48893773],\n","        [-3.88736595, -4.67800751]]),\n"," array(['female', 'male'], dtype='<U6'),\n"," array([[-11.09424018, -22.73291741],\n","        [-15.27968966, -12.41764665]]))"]},"metadata":{},"execution_count":17}],"source":["class NaiveBayes(object):\n","\n","    def __init__(self, multinomial=True):\n","        if multinomial:\n","            self.train = fit_multinomial_NB\n","            self.pred = predict_multinomial_NB1\n","        else:\n","            self.train = fit_gaussian_NB\n","            self.pred = predict_gaussian_NB1\n","\n","    def fit(self, X, Y):\n","        self.Theta = self.train(X, Y)\n","\n","    def predict(self, X, add_prior=True, prob=False, alpha=0.):\n","        Y_pred = []\n","        for i in range(len(X)):\n","            Y_pred.append(predict_NB1(\n","                X[i,:], self.Theta, self.pred, add_prior=add_prior, alpha=alpha\n","                ))\n","\n","        Y_pred = np.array(Y_pred)\n","\n","        if prob:\n","            return Y_pred\n","\n","        return np.choose(np.argmax(Y_pred, axis=1), self.Theta['prior']['vocab'])\n","\n","#=====================================================================\n","# UNIT TEST\n","#=====================================================================\n","# Result:\n","# (array(['yes', 'yes', 'no'], dtype='<U3'),\n","#  array([[-3.82235951, -3.01795347],\n","#         [-4.9209718 , -4.40424783],\n","#         [-2.50060367, -3.59331761]]),\n","#  array(['female', 'male'], dtype='<U6'),\n","#  array([[ -9.901093  , -21.53977023],\n","#         [-14.08654248, -11.22449947]]))\n","#---------------------------------------------------------------------\n","\n","multinomial_nb = NaiveBayes()\n","multinomial_nb.fit(Xplay, Yplay)\n","\n","gaussian_nb = NaiveBayes(multinomial=False)\n","gaussian_nb.fit(Xperson, Yperson)\n","\n","X_t1 = np.array([\n","    ['rainy', 'cool', 'normal', 'yes'],\n","    ['snowy', 'cool', 'normal', 'yes'],\n","    ['sunny', 'hot' , 'high', 'no']\n","])\n","\n","X_t2 = np.array([\n","    [183., 59., 20.],\n","    [175., 65., 30.]\n","])\n","\n","\n","multinomial_nb.predict(X_t1, alpha=1.), \\\n","    multinomial_nb.predict(X_t1, alpha=1., prob=True), \\\n","    gaussian_nb.predict(X_t2), \\\n","    gaussian_nb.predict(X_t2, prob=True)"]},{"cell_type":"markdown","metadata":{"id":"4JOtHomSUk2N"},"source":["## II. Application and Analysis\n","\n","In this section, we will test different concepts by running an experiment, formulating a hypothesis and trying to justify it.\n","\n","### II.1. Prior probability\n","\n","We want to test the effect of prior probability.\n","To do this, we trained two models:\n","1. With prior probability\n","1. Without prior probability (It considers a uniform distribution of classes)\n","\n","To test whether the models have adapted well to the training dataset, we will test them on the same dataset and calculate the classification ratio.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xB5rJR6_Uk2N","outputId":"e797b62e-1dc8-4450-e33b-ac2e0c25c8cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Considring prior probability\n","              precision    recall  f1-score   support\n","\n","          no       1.00      0.80      0.89         5\n","         yes       0.90      1.00      0.95         9\n","\n","    accuracy                           0.93        14\n","   macro avg       0.95      0.90      0.92        14\n","weighted avg       0.94      0.93      0.93        14\n","\n","No prior probability\n","              precision    recall  f1-score   support\n","\n","          no       0.67      0.80      0.73         5\n","         yes       0.88      0.78      0.82         9\n","\n","    accuracy                           0.79        14\n","   macro avg       0.77      0.79      0.78        14\n","weighted avg       0.80      0.79      0.79        14\n","\n"]}],"source":["nb_withPrior     = CategoricalNB(alpha=1.0, fit_prior=True )\n","nb_noPrior       = CategoricalNB(alpha=1.0, fit_prior=False)\n","\n","enc         = OrdinalEncoder()\n","Xplay_tf    = enc.fit_transform(Xplay)\n","nb_withPrior.fit(Xplay_tf, Yplay)\n","nb_noPrior.fit(Xplay_tf, Yplay)\n","\n","Ypred_withPrior = nb_withPrior.predict(Xplay_tf)\n","Ypred_noPrior = nb_noPrior.predict(Xplay_tf)\n","\n","\n","print( 'Considring prior probability'  )\n","print(classification_report(Yplay, Ypred_withPrior))\n","\n","print( 'No prior probability'  )\n","print(classification_report(Yplay, Ypred_noPrior))"]},{"cell_type":"markdown","metadata":{"id":"Nfyc4Bt8Uk2N"},"source":["**TODO: Analyze the results**\n","\n","1. What do you notice, indicating if prior probability is useful in this case?\n","1. How does this probability affect the outcome?\n","1. When are we sure that using this probability is unnecessary?\n","\n","**Answer**\n","\n","1. So basically, considering the prior probability makes a big difference. The model's accuracy jumps from 0.79 to 0.93, and metrics like recall, F1-scores, and precision improve for both classes. This happens because including the prior probability helps the model better match the actual distribution of the classes, leading to more accurate results. On the other hand, the model that doesn't use prior probability struggles to distinguish between \"yes\" and \"no\" because it treats them as if they occur equally, which isn't the case.\n","1. when we incorporate prior probability, the model can handle class imbalance better. It adjusts the likelihood of each class based on their actual distribution, which boosts accuracy and overall performance. Without this adjustment, the model assumes all classes are equally likely, which can really mess up predictions, especially when the classes aren't evenly distributed. Basically, without prior probability, the model might underestimate common classes and overestimate rare ones, leading to poor predictions\n","1. classes are uniformly distributed."]},{"cell_type":"markdown","metadata":{"id":"kqQC8Mc_Uk2N"},"source":["### II.2. Smoothing\n","\n","We want to test the Lidstone smoothing's effect.\n","To do this, we trained three models:\n","1. alpha = 1 (Laplace smoothing)\n","1. alpha = 0.5\n","1. alpha = 0 (without smoothing)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5lS7utaUk2O","outputId":"3399a9c7-2ce8-4e8b-ed51-c1ea598cd2f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Alpha = 1.0\n","              precision    recall  f1-score   support\n","\n","          no       1.00      0.80      0.89         5\n","         yes       0.90      1.00      0.95         9\n","\n","    accuracy                           0.93        14\n","   macro avg       0.95      0.90      0.92        14\n","weighted avg       0.94      0.93      0.93        14\n","\n","Alpha = 0.5\n","              precision    recall  f1-score   support\n","\n","          no       1.00      0.80      0.89         5\n","         yes       0.90      1.00      0.95         9\n","\n","    accuracy                           0.93        14\n","   macro avg       0.95      0.90      0.92        14\n","weighted avg       0.94      0.93      0.93        14\n","\n","Alpha = 0.0\n","              precision    recall  f1-score   support\n","\n","          no       1.00      0.80      0.89         5\n","         yes       0.90      1.00      0.95         9\n","\n","    accuracy                           0.93        14\n","   macro avg       0.95      0.90      0.92        14\n","weighted avg       0.94      0.93      0.93        14\n","\n"]},{"name":"stderr","output_type":"stream","text":["/opt/env/ml/lib/python3.10/site-packages/sklearn/naive_bayes.py:1504: RuntimeWarning: divide by zero encountered in log\n","  np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n"]}],"source":["NBC_10 = CategoricalNB(alpha = 1.0 )\n","NBC_05 = CategoricalNB(alpha = 0.5 )\n","NBC_00 = CategoricalNB(alpha = 0.0 )\n","\n","NBC_10.fit( Xplay_tf,   Yplay )\n","NBC_05.fit( Xplay_tf,   Yplay )\n","NBC_00.fit( Xplay_tf,   Yplay )\n","\n","Y_10   = NBC_10.predict(Xplay_tf)\n","Y_05   = NBC_05.predict(Xplay_tf)\n","Y_00   = NBC_00.predict(Xplay_tf)\n","\n","\n","print(                'Alpha = 1.0'                        )\n","print(classification_report(Yplay, Y_10, zero_division=0.0))\n","\n","print(                'Alpha = 0.5'                        )\n","print(classification_report(Yplay, Y_05, zero_division=0.0))\n","\n","print(                'Alpha = 0.0'                        )\n","print(classification_report(Yplay, Y_00, zero_division=0.0))\n"]},{"cell_type":"markdown","metadata":{"id":"4BViAG2RUk2O"},"source":["**TODO: Analyze the results**\n","\n","1. What do you notice, indicating if smoothing affects performance in this case?\n","1. Based on the past answeer, Why?\n","1. Why do we get a \"RuntimeWarning: divide by zero\" error?\n","1. What is the benefit of smoothing (generally; not just for this case)?\n","\n","**Answer**\n","\n","1. So, it turns out that smoothing doesn't have a big impact on the model's performance; Whether we set α to 0, 0.5, or 1, the accuracy, precision, recall, and f1-score stay pretty much the same. hwich is means that changing the smoothing parameter don't really affect the overall results.\n","1. So, the reason smoothing doesn't make much of a difference is because the dataset is well balanced and has no missing features or zero counts for any class. Since there are no zero probabilities when calculating likelihoods, smoothing doesn't really change the outcomes. That's why the performance metrics stay consistent across different values of alpha.\n","1. If alpha is set to 0 and a feature is completely absent in a class during training, its probability drops to zero. This creates a problem when the model needs to compute the logarithm of zero (log 0), leading to a divide-by-zero warning. To avoid this issue, smoothing is applied. By adding a small alpha value for each class, smoothing ensures that no probability is zero, thus preventing any divide-by-zero errors.\n","1. Smoothing is essential because it protects the model from problems related to missing data. Without it, unseen data can lead to zero probabilities, causing the model to fail. In real-world scenarios, smoothing is crucial as it adds a small value to each probability, ensuring the model remains robust and performs better overall."]},{"cell_type":"markdown","metadata":{"id":"0BYy5E1wUk2O"},"source":["### II.3. Naive Bayes performance\n","\n","Naive Bayes is known to generate powerful models when it comes to classifying textual documents.\n","We want to test this proposition using spam detection over [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset) dataset.\n","\n","Each message is represented using term frequency (TF), where a word is considered as a feature.\n","In this case, a message is represented by a vector of frequencies (how many times each word appeared in the message).\n","We want to compare these models:\n","1. Multinomial Naive Bayes (MNB)\n","1. Gaussian Naive Bayes (GNB)\n","1. Logistic Regression (LR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uABQ7UkBUk2O","outputId":"1632eb2d-00fb-4b7d-d397-0872c74d101c"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Ok lar... Joking wif u oni...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","      <td>spam</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>U dun say so early hor... U c already then say...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","      <td>ham</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text class\n","0  Go until jurong point, crazy.. Available only ...   ham\n","1                      Ok lar... Joking wif u oni...   ham\n","2  Free entry in 2 a wkly comp to win FA Cup fina...  spam\n","3  U dun say so early hor... U c already then say...   ham\n","4  Nah I don't think he goes to usf, he lives aro...   ham"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# reading the dataset\n","messages = pd.read_csv('data/spam.csv', encoding='latin-1')\n","# renaming features: text and class\n","messages = messages.rename(columns={'v1': 'class', 'v2': 'text'})\n","# keeping only these two features\n","messages = messages.filter(['text', 'class'])\n","\n","messages.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2jXqnp4fUk2P","outputId":"4c7922cc-e330-4822-aa72-af3aa04b2dec"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Algorithm</th>\n","      <th>Train time</th>\n","      <th>Test time</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Multinomial Naive Bayes (MNB)</td>\n","      <td>0.288778</td>\n","      <td>0.021871</td>\n","      <td>0.987179</td>\n","      <td>0.927711</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Gaussian Naive Bayes  (GNB)</td>\n","      <td>0.420813</td>\n","      <td>0.095978</td>\n","      <td>0.616667</td>\n","      <td>0.891566</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Logistic Regression (LR)</td>\n","      <td>0.551115</td>\n","      <td>0.022939</td>\n","      <td>0.986111</td>\n","      <td>0.855422</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       Algorithm  Train time  Test time  Precision    Recall\n","0  Multinomial Naive Bayes (MNB)    0.288778   0.021871   0.987179  0.927711\n","1    Gaussian Naive Bayes  (GNB)    0.420813   0.095978   0.616667  0.891566\n","2       Logistic Regression (LR)    0.551115   0.022939   0.986111  0.855422"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["models = [\n","    MultinomialNB(),\n","    GaussianNB(),\n","    LogisticRegression(solver='lbfgs'),\n","    #solver=sag is slower; so I chose the fastest\n","]\n","\n","algos = [\n","    'Multinomial Naive Bayes (MNB)',\n","    'Gaussian Naive Bayes  (GNB)',\n","    'Logistic Regression (LR)',\n","]\n","\n","perf = {\n","    'train_time': [],\n","    'test_time' : [],\n","    'recall'    : [],\n","    'precision' : []\n","}\n","\n","\n","msg_train, msg_test, Y_train, Y_test = train_test_split(messages['text'] ,\n","                                                        messages['class'],\n","                                                        test_size    = 0.2,\n","                                                        random_state = 0  )\n","\n","count_vectorizer = CountVectorizer()\n","X_train          = count_vectorizer.fit_transform(msg_train).toarray()\n","X_test           = count_vectorizer.transform    (msg_test ).toarray()\n","\n","\n","for model in models:\n","    # ==================================\n","    # TRAIN\n","    # ==================================\n","    start_time = timeit.default_timer()\n","    model.fit(X_train, Y_train)\n","    perf['train_time'].append(timeit.default_timer() - start_time)\n","\n","    # ==================================\n","    # TEST\n","    # ==================================\n","    start_time = timeit.default_timer()\n","    Y_pred     = model.predict(X_test)\n","    perf['test_time'].append(timeit.default_timer() - start_time)\n","\n","    # ==================================\n","    # PERFORMANCE\n","    # ==================================\n","    # In here, we are interrested in \"spam\" class which is our positive class\n","    perf['precision'].append(precision_score(Y_test, Y_pred, pos_label='spam'))\n","    perf['recall'   ].append(recall_score   (Y_test, Y_pred, pos_label='spam'))\n","\n","\n","pd.DataFrame({\n","    'Algorithm' : algos,\n","    'Train time': perf['train_time'],\n","    'Test time' : perf['test_time'],\n","    'Precision' : perf['precision'],\n","    'Recall'    : perf['recall']\n","})"]},{"cell_type":"markdown","metadata":{"id":"rudmdNkcUk2P"},"source":["**TODO: Analyze the results**\n","\n","1. What do you notice about training time? (order the algorithms)\n","1. Why did we get these results based on the algorithms? (discuss each algorithm with respect to training time)\n","1. What do you notice about the testing time? (order the algorithms)\n","1. Why did we get these results based on the algorithms? (discuss each algorithm with respect to testing time)\n","1. Why is the Gaussian model less efficient than the multinomial based on the nature of the two algorithms?\n","1. Why is the Gaussian model less efficient than the multinomial based on the nature of the problem/data?\n","1. How Multinomial NB's implementation affect the training/test time? (store statistics vs. store probabilities)\n","1. Which one is more adequate for updating the model with new data? explain.\n","\n","**Answer**\n","\n","1.  Speaking about the training time, we order the models from fastest to slowest : Multinominal naive bayes , gaussian naive baiyes and finally logistic naive bayes.\n","\n","1. Naive bayes algorithm is know for its simplicity and speed comparing to Logistic regression , especially on small datasets, that's why the training was faster for the naive bayes models.\n","And the algorithm differs too between MNB and GNB , the first one relies on simple probaility calculations , while the other calculates variances and means, which explains why MNB was faster than GNB.\n","1.  For testing time, we got a slightly different order comparing to the training one, from fastest to slowest we got : multinominal naive bayes , logistic regression then\n","gaussian naive bayes.\n","1. MNB is fastest because it only needs to count word frequencies and calculate simple probabilities and GNB is slower because it needs to calculate means and variances for each feature.\n","1.  the main reason is the computational algorithm used by each model, while MNB uses simple calculations for probabilities and frequencies , GNB has to calculate means and variances.\n","1. the guassian model was less efficient than the multinominal model, The main difference between the two lies in the assumptions they make about the distribution of features.\n","Multinominal naive bayes assumes that the features have a multinominal distibution, meaning they represent the frequencies with which certain events occur over a fixed number of trials. This assumption is appropriate for discrete data, just like our study case about spam and non spam texts.\n","On the other hand, GNB assumes that the data follows a guassian distibution, where it is represented in a continious way. This assumption is not suitable\n","for our dataset since the features represent discrete counts of words rather than continuous measurements.\n","1. Multinominal naive bayes stores simple statistics which are simple to update comparing to the two other models\n","1. MNB is better for updating with new data because it can incrementally update counts. Plus it doesn't need to recompute complex statistics like variance in the case of GNB.\n","- New words can be added without restructuring the model\n","- GNB would need to recompute means/variances from scratch when new data arrives"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stXv0cwZUk2P","outputId":"441110dd-94fb-4f20-f906-475ec81a968b"},"outputs":[{"name":"stdout","output_type":"stream","text":["  _____    __                                              _               \n"," |_   _|  / _|                                            | |              \n","   | |   | |_     _   _    ___    _   _      __ _    ___  | |_             \n","   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \n","  _| |_  | |     | |_| | | (_) | | |_| |   | (_| | |  __/ | |_             \n"," |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \n","                   __/ |                     __/ |                         \n","                  |___/                     |___/                          \n","  _     _       _            __                                            \n"," | |   | |     (_)          / _|                 _                         \n"," | |_  | |__    _   ___    | |_    __ _   _ __  (_)                        \n"," | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \n"," | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \n","  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \n","                                                |/                         \n","                                                                           \n","                                                                           \n","                                                                           \n","  _   _    ___    _   _      __ _   _ __    ___                            \n"," | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \n"," | |_| | | (_) | | |_| |   | (_| | | |    |  __/                           \n","  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \n","   __/ |                                                                   \n","  |___/                                                                    \n","                    _                                                __    \n","                   | |                                            _  \\ \\   \n","  _ __     ___     | |__    _   _   _ __ ___     __ _   _ __     (_)  | |  \n"," | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \n"," | | | | | (_) |   | | | | | |_| | | | | | | | | (_| | | | | |    _   | |  \n"," |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \n","                                                                     /_/   \n","                                                                           \n"]}],"source":["print(\"  _____    __                                              _               \")\n","print(\" |_   _|  / _|                                            | |              \")\n","print(\"   | |   | |_     _   _    ___    _   _      __ _    ___  | |_             \")\n","print(\"   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \")\n","print(\"  _| |_  | |     | |_| | | (_) | | |_| |   | (_| | |  __/ | |_             \")\n","print(\" |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \")\n","print(\"                   __/ |                     __/ |                         \")\n","print(\"                  |___/                     |___/                          \")\n","print(\"  _     _       _            __                                            \")\n","print(\" | |   | |     (_)          / _|                 _                         \")\n","print(\" | |_  | |__    _   ___    | |_    __ _   _ __  (_)                        \")\n","print(\" | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \")\n","print(\" | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \")\n","print(\"  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \")\n","print(\"                                                |/                         \")\n","print(\"                                                                           \")\n","print(\"                                                                           \")\n","print(\"                                                                           \")\n","print(\"  _   _    ___    _   _      __ _   _ __    ___                            \")\n","print(\" | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \")\n","print(\" | |_| | | (_) | | |_| |   | (_| | | |    |  __/                           \")\n","print(\"  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \")\n","print(\"   __/ |                                                                   \")\n","print(\"  |___/                                                                    \")\n","print(\"                    _                                                __    \")\n","print(\"                   | |                                            _  \\ \\   \")\n","print(\"  _ __     ___     | |__    _   _   _ __ ___     __ _   _ __     (_)  | |  \")\n","print(\" | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \")\n","print(\" | | | | | (_) |   | | | | | |_| | | | | | | | | (_| | | | | |    _   | |  \")\n","print(\" |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \")\n","print(\"                                                                     /_/   \")\n","print(\"                                                                           \")"]}],"metadata":{"kernelspec":{"display_name":"ml","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}